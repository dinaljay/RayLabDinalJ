
import numpy as np
import os.path as path
import matplotlib.pyplot as plt
import pandas as pd
import sys

## Initialize features

dhi_features = ["dti_adc_map", "dti_axial_map", "dti_fa_map", "fiber1_axial_map", "fiber1_fa_map",
    "fiber1_radial_map", "fiber_fraction_map", "hindered_fraction_map", "restricted_fraction_map",
                "water_fraction_map", "axon_volume", "inflammation_volume"]

controls = np.array([4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20])
mild_cm_subjects = np.array([1,2,3,4,10,15,16,18,19,21,23,24,26,28,29,31,32,36,38,40,42,43,44,45,46])
moderate_cm_subjects = np.array([5,6,9,12,13,14,20,22,25,27,30,34,37,41,47])

all_cm = np.concatenate((mild_cm_subjects,moderate_cm_subjects),axis=0)

control_ids = np.array([0]*len(controls))
csm_ids = np.array([1]*len(all_cm))

all_ids = np.concatenate((control_ids,csm_ids),axis=0)

## Load Data

url = '/media/functionalspinelab/RAID/Data/Dinal/Pycharm_Data/White_Matter/Pycharm_Data_Voxel/DBSI_CSV_Data/all_patients_all_features_data.csv'

all_data = pd.read_csv(url, header=0)

X = all_data.drop('Group', axis=1)
y = all_data['Group']
#sys.exit()

# Scale data

from sklearn import preprocessing

X_scaled = preprocessing.scale(X)

#Recursive feature elimination

# Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV

logreg = LogisticRegression(random_state=0)

rfe = RFECV(logreg,min_features_to_select=35,cv=4)
rfe = rfe.fit(X_scaled, y)

cols =[]
i=0
for value in rfe.ranking_:
    if value==1:
        cols.append(i)
    i+=1

cols_to_drop=[]
columns = X.columns

for i in cols:
    cols_to_drop.append(columns[i])

X = X.drop(cols_to_drop, axis=1)

#Rescale data
X_scaled = preprocessing.scale(X)


# Logistic regression

from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import train_test_split

#because CSM patients are over sampled, we use SMOTE to balance out the classes

#from imblearn.over_sampling import SMOTE

#os = SMOTE(random_state=0)


#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
#columns_x = X_train.columns
#columns_y = y_train.columns
#os_data_X,os_data_y=os.fit_sample(X_train, y_train)
#os_data_X = pd.DataFrame(data=os_data_X,columns=columns_x )
#os_data_y= pd.DataFrame(data=os_data_y,columns=columns_y)

# Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
loocv = LeaveOneOut()

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
logreg = LogisticRegression(random_state=0)

clf = logreg.fit(X_scaled, y)

results = cross_val_score(logreg, X_scaled, y, cv=loocv)
print("Accuracy: %.3f%%" % (results.mean()*100))

sys.exit()

#Confusion matrix

from sklearn.metrics import confusion_matrix

#cf_matrix = confusion_matrix(y_test,y_pred)
#print("Confusion matrix:", cf_matrix)

#sys.exit()
#Classification report

from sklearn.metrics import classification_report
print("Classification report:", classification_report(y_test, y_pred))

# ROC Curve

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()







